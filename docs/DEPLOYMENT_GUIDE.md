# üöÄ Real-Time Deployment Guide for Office Use

## **Created by Kavi**

---

## **Table of Contents**
1. [Quick Start (Small Office)](#scenario-1-quick-start-small-office)
2. [Enterprise Deployment (Medium-Large Office)](#scenario-2-enterprise-deployment)
3. [Full SIEM Integration (Large Organization)](#scenario-3-full-siem-integration)
4. [Real-Time Alert Ingestion](#real-time-alert-ingestion)
5. [Integration with Existing Tools](#integration-options)
6. [Monitoring & Alerting](#monitoring--alerting)
7. [Security Considerations](#security-considerations)

---

## **SCENARIO 1: Quick Start (Small Office)**

### **Best for:** 1-50 employees, basic security monitoring

### **Architecture:**
```
Office Network Logs ‚Üí Log Collector ‚Üí AI Explainer ‚Üí Dashboard
                                           ‚Üì
                                    Email/Slack Alerts
```

### **Step-by-Step Setup:**

#### **1. Collect Network Logs**

**Option A: Windows Event Logs (Active Directory)**
```powershell
# Export failed login attempts
Get-EventLog -LogName Security -InstanceId 4625 -Newest 1000 |
Export-Csv -Path "C:\security_logs\failed_logins.csv"
```

**Option B: Firewall Logs (pfSense, FortiGate, etc.)**
- Configure syslog forwarding to your server
- IP: `your-ai-server-ip:514`

**Option C: Antivirus Logs (Windows Defender, Symantec)**
```powershell
# Export Defender alerts
Get-MpThreatDetection | Export-Csv defender_alerts.csv
```

#### **2. Create Log Parser Script**

Create `scripts/parse_office_logs.py`:
```python
import pandas as pd
from datetime import datetime
import os

def parse_windows_security_logs(log_file):
    """Convert Windows Event Logs to AI-ready format"""

    # Read raw logs
    df = pd.read_csv(log_file)

    # Transform to alert format
    alerts = []
    for _, row in df.iterrows():
        alert = {
            'alert_id': str(uuid.uuid4()),
            'timestamp': datetime.now().isoformat(),
            'source_ip': extract_ip(row['Message']),
            'source_country': 'US',  # Use GeoIP lookup
            'destination_ip': 'internal',
            'destination_port': 3389 if 'RDP' in row['Message'] else 445,
            'protocol': 'TCP',
            'failed_login_attempts': 1 if row['EventID'] == 4625 else 0,
            'process_executed': extract_process(row['Message']),
            'off_hours_activity': is_off_hours(row['TimeCreated']),
            # ... other fields
        }
        alerts.append(alert)

    return pd.DataFrame(alerts)

def is_off_hours(timestamp):
    """Check if activity happened outside 9-5"""
    hour = timestamp.hour
    return hour < 9 or hour > 17 or timestamp.weekday() > 4

# Auto-run every 5 minutes
if __name__ == "__main__":
    while True:
        logs = parse_windows_security_logs("C:/security_logs/events.csv")
        logs.to_csv("data/raw/alerts.csv", mode='a', header=False)
        time.sleep(300)  # 5 minutes
```

#### **3. Set Up Auto-Analysis**

Create `scripts/auto_analyzer.py`:
```python
import time
import pandas as pd
from src.ingestion.alert_loader import AlertLoader
from src.ml_engine.model_predictor import ModelPredictor
from src.xai.shap_explainer import SHAPExplainer
from src.llm_engine.claude_client import ClaudeExplainer
import smtplib
from email.message import EmailMessage

def analyze_new_alerts():
    """Process new alerts every minute"""

    # Load alerts
    df = AlertLoader.load_csv("data/raw/alerts.csv")

    # Get unprocessed alerts
    new_alerts = df[df['processed'] == False]

    for _, alert in new_alerts.iterrows():
        # Run through pipeline
        prediction = model.predict(alert)
        explanation = shap_explainer.explain(alert)
        llm_text = claude.generate_explanation(prediction, explanation)

        # Send alert if malicious or suspicious
        if prediction['verdict'] in ['malicious', 'suspicious']:
            send_email_alert(alert, prediction, llm_text)
            send_slack_alert(alert, prediction, llm_text)

        # Mark as processed
        mark_processed(alert['alert_id'])

def send_email_alert(alert, prediction, explanation):
    """Send email notification"""
    msg = EmailMessage()
    msg['Subject'] = f"üö® SECURITY ALERT: {prediction['verdict'].upper()}"
    msg['From'] = "security-ai@yourcompany.com"
    msg['To'] = "soc-team@yourcompany.com"

    msg.set_content(f"""
    SECURITY ALERT DETECTED

    Verdict: {prediction['verdict'].upper()}
    Confidence: {prediction['confidence']*100:.1f}%

    Alert Details:
    - Source IP: {alert['source_ip']}
    - Process: {alert['process_executed']}
    - Timestamp: {alert['timestamp']}

    AI Explanation:
    {explanation}

    View full details: http://your-dashboard-url/alert/{alert['alert_id']}

    ---
    This alert was generated by AI Security Decision Explainer
    Created by Kavi
    """)

    with smtplib.SMTP('smtp.gmail.com', 587) as smtp:
        smtp.starttls()
        smtp.login('your-email@gmail.com', 'app-password')
        smtp.send_message(msg)

def send_slack_alert(alert, prediction, explanation):
    """Send Slack notification"""
    import requests

    webhook_url = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

    message = {
        "text": f"üö® *SECURITY ALERT: {prediction['verdict'].upper()}*",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"üö® Security Alert: {prediction['verdict'].upper()}"
                }
            },
            {
                "type": "section",
                "fields": [
                    {"type": "mrkdwn", "text": f"*Confidence:*\n{prediction['confidence']*100:.1f}%"},
                    {"type": "mrkdwn", "text": f"*Source IP:*\n{alert['source_ip']}"},
                    {"type": "mrkdwn", "text": f"*Process:*\n{alert['process_executed']}"},
                    {"type": "mrkdwn", "text": f"*Time:*\n{alert['timestamp']}"}
                ]
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*AI Explanation:*\n{explanation}"
                }
            },
            {
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "View Details"},
                        "url": f"http://your-dashboard/alert/{alert['alert_id']}"
                    }
                ]
            }
        ]
    }

    requests.post(webhook_url, json=message)

# Run continuously
if __name__ == "__main__":
    print("üõ°Ô∏è AI Security Monitor Started - Created by Kavi")
    while True:
        try:
            analyze_new_alerts()
            time.sleep(60)  # Check every minute
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(60)
```

#### **4. Deploy Dashboard**

```bash
# Start the dashboard (accessible to SOC team)
python scripts/run_dashboard.py

# Access from office network
http://your-server-ip:8000
```

#### **5. Set Up Windows Service (Auto-Start)**

Create `install_service.bat`:
```batch
@echo off
echo Installing AI Security Monitor Service...

# Create Windows Task Scheduler job
schtasks /create /tn "AI_Security_Monitor" /tr "python D:\ai-security-decision-explainer\scripts\auto_analyzer.py" /sc onstart /ru SYSTEM

echo Service installed! The AI will start on system boot.
```

---

## **SCENARIO 2: Enterprise Deployment (Medium-Large Office)**

### **Best for:** 50-500 employees, multiple office locations

### **Architecture:**
```
Firewalls, EDR, Email Security
        ‚Üì
SYSLOG Aggregator (rsyslog/syslog-ng)
        ‚Üì
Kafka Message Queue
        ‚Üì
AI Processing Cluster (3+ servers)
        ‚Üì
PostgreSQL Database
        ‚Üì
Web Dashboard + API
        ‚Üì
Email, Slack, PagerDuty, Teams
```

### **Infrastructure Setup:**

#### **1. Deploy with Docker Compose**

Create `docker-compose.yml`:
```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: security_alerts
      POSTGRES_USER: ai_security
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  # Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # Kafka for Real-Time Streaming
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    ports:
      - "9092:9092"

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  # AI Processing Engine
  ai_processor:
    build: .
    command: python scripts/realtime_processor.py
    environment:
      - DATABASE_URL=postgresql://ai_security:${DB_PASSWORD}@postgres/security_alerts
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - kafka
      - redis
    deploy:
      replicas: 3  # Scale horizontally

  # Web Dashboard
  dashboard:
    build: .
    command: python scripts/run_dashboard.py
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://ai_security:${DB_PASSWORD}@postgres/security_alerts
    depends_on:
      - postgres

  # Alert Notifier
  notifier:
    build: .
    command: python scripts/alert_notifier.py
    environment:
      - SMTP_SERVER=${SMTP_SERVER}
      - SLACK_WEBHOOK=${SLACK_WEBHOOK}
      - PAGERDUTY_KEY=${PAGERDUTY_KEY}
    depends_on:
      - redis

volumes:
  postgres_data:
```

#### **2. Deploy to Production**

```bash
# Set environment variables
export DB_PASSWORD="secure_password_here"
export SMTP_SERVER="smtp.office365.com"
export SLACK_WEBHOOK="https://hooks.slack.com/..."

# Deploy
docker-compose up -d

# Scale AI processors
docker-compose up -d --scale ai_processor=5
```

#### **3. Configure SIEM Integration**

**For Splunk:**
```python
# scripts/splunk_integration.py
import splunklib.client as client
import splunklib.results as results

# Connect to Splunk
service = client.connect(
    host='splunk.yourcompany.com',
    port=8089,
    username='admin',
    password='changeme'
)

# Real-time search for security events
search_query = """
search index=security sourcetype=firewall OR sourcetype=windows:security
| where EventCode=4625 OR action="blocked"
| table _time, src_ip, dest_ip, user, action, process
"""

# Stream results to AI
job = service.jobs.create(search_query, earliest_time="-1m", search_mode="realtime")

for result in results.ResultsReader(job.results()):
    # Convert to alert format
    alert = convert_splunk_to_alert(result)

    # Send to Kafka for processing
    producer.send('security-alerts', alert)
```

**For Microsoft Sentinel:**
```python
# scripts/sentinel_integration.py
from azure.monitor.query import LogsQueryClient
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()
client = LogsQueryClient(credential)

# Query Sentinel
query = """
SecurityAlert
| where TimeGenerated > ago(1m)
| where AlertSeverity in ("High", "Medium")
| project TimeGenerated, AlertName, Entities, RemediationSteps
"""

response = client.query_workspace(
    workspace_id="your-workspace-id",
    query=query,
    timespan=timedelta(minutes=1)
)

for row in response.tables[0].rows:
    alert = convert_sentinel_to_alert(row)
    kafka_producer.send('security-alerts', alert)
```

---

## **SCENARIO 3: Full SIEM Integration (Large Organization)**

### **Best for:** 500+ employees, SOC team, compliance requirements

### **Professional Deployment:**

```
Corporate Network
    ‚îú‚îÄ‚îÄ Firewalls (Palo Alto, Fortinet)
    ‚îú‚îÄ‚îÄ EDR (CrowdStrike, SentinelOne)
    ‚îú‚îÄ‚îÄ Email Security (Proofpoint, Mimecast)
    ‚îú‚îÄ‚îÄ Cloud Logs (AWS CloudTrail, Azure Monitor)
    ‚îî‚îÄ‚îÄ Identity (Okta, Azure AD)
           ‚Üì
    SIEM (Splunk, QRadar, Sentinel)
           ‚Üì
    AI Security Decision Explainer (This System)
           ‚Üì
    ‚îú‚îÄ‚îÄ Real-Time Dashboard
    ‚îú‚îÄ‚îÄ SOAR Integration (Phantom, Demisto)
    ‚îú‚îÄ‚îÄ Ticketing (Jira, ServiceNow)
    ‚îî‚îÄ‚îÄ Alerting (PagerDuty, Slack, Teams)
```

### **Implementation:**

#### **1. Install on Kubernetes (High Availability)**

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-security-explainer
spec:
  replicas: 5
  selector:
    matchLabels:
      app: ai-security
  template:
    metadata:
      labels:
        app: ai-security
    spec:
      containers:
      - name: ai-processor
        image: your-registry/ai-security-explainer:latest
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        env:
        - name: KAFKA_BROKERS
          value: "kafka-cluster:9092"
---
apiVersion: v1
kind: Service
metadata:
  name: ai-dashboard
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 8000
  selector:
    app: ai-security
```

#### **2. Set Up Alert Routing**

```python
# config/alert_routing.py

ALERT_ROUTING_RULES = {
    'malicious': {
        'channels': ['pagerduty', 'slack', 'email', 'sms'],
        'priority': 'P1',
        'auto_create_ticket': True,
        'recipients': ['soc-team@company.com', 'security-lead@company.com']
    },
    'suspicious': {
        'channels': ['slack', 'email'],
        'priority': 'P2',
        'auto_create_ticket': True,
        'recipients': ['soc-team@company.com']
    },
    'benign': {
        'channels': ['dashboard'],  # Only show in dashboard
        'priority': 'P3',
        'auto_create_ticket': False
    }
}
```

---

## **Real-Time Alert Ingestion**

### **Method 1: Syslog (Universal)**

```python
# scripts/syslog_receiver.py
import socketserver

class SyslogHandler(socketserver.BaseRequestHandler):
    def handle(self):
        data = self.request[0].strip()

        # Parse syslog message
        alert = parse_syslog(data)

        # Send to AI pipeline
        process_alert(alert)

# Start syslog server
server = socketserver.UDPServer(('0.0.0.0', 514), SyslogHandler)
server.serve_forever()
```

### **Method 2: API Endpoint**

```python
# Add to src/dashboard/routes.py

@router.post("/api/ingest/alert")
async def ingest_real_time_alert(alert: dict):
    """
    Receive alerts from external systems

    curl -X POST http://your-server/api/ingest/alert \
         -H "Authorization: Bearer YOUR_API_KEY" \
         -d '{"source_ip": "192.168.1.100", ...}'
    """

    # Validate alert
    if not validate_alert(alert):
        raise HTTPException(400, "Invalid alert format")

    # Process through AI pipeline
    result = await process_alert_async(alert)

    return {"status": "processed", "verdict": result['verdict']}
```

### **Method 3: File Watcher**

```python
# scripts/file_watcher.py
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class LogFileHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.src_path.endswith('.log'):
            # Read new lines
            new_alerts = parse_log_file(event.src_path)

            for alert in new_alerts:
                process_alert(alert)

observer = Observer()
observer.schedule(LogFileHandler(), path='/var/log/security/', recursive=True)
observer.start()
```

---

## **Integration Options**

### **1. Splunk App**
```bash
# Create Splunk app
mkdir -p splunk_app/bin
cp scripts/splunk_integration.py splunk_app/bin/

# Install in Splunk
splunk install app splunk_app
```

### **2. QRadar Custom Action**
```python
# QRadar REST API integration
import requests

qradar_url = "https://qradar.company.com/api"
headers = {"SEC": "your-api-token"}

# Fetch offenses
offenses = requests.get(f"{qradar_url}/siem/offenses", headers=headers).json()

for offense in offenses:
    # Send to AI for analysis
    verdict = analyze_offense(offense)

    # Update QRadar with AI verdict
    requests.post(f"{qradar_url}/siem/offenses/{offense['id']}/notes",
                  headers=headers,
                  json={"note_text": f"AI Verdict: {verdict}"})
```

### **3. Microsoft Sentinel Playbook**
```json
{
  "definition": {
    "$schema": "https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#",
    "actions": {
      "Send_to_AI": {
        "type": "Http",
        "inputs": {
          "method": "POST",
          "uri": "https://your-ai-server.com/api/ingest/alert",
          "body": "@triggerBody()?['Entities']"
        }
      }
    }
  }
}
```

---

## **Monitoring & Alerting**

### **System Health Dashboard**

```python
# Create /api/health endpoint
@router.get("/api/health")
def health_check():
    return {
        "status": "healthy",
        "components": {
            "ml_model": model_status(),
            "database": db_status(),
            "kafka": kafka_status(),
            "queue_length": get_queue_length()
        },
        "metrics": {
            "alerts_processed_today": get_daily_count(),
            "average_processing_time_ms": get_avg_time(),
            "malicious_detected": get_malicious_count()
        }
    }
```

### **Grafana Dashboard**
- Model performance metrics
- Alert processing rate
- False positive trends
- System resource usage

---

## **Security Considerations**

### **1. Network Isolation**
```bash
# Deploy in isolated VLAN
VLAN: 100 (Security Tools)
Firewall Rules:
  - Allow: SIEM ‚Üí AI Server (port 8000)
  - Allow: AI Server ‚Üí Database (port 5432)
  - Deny: ALL other traffic
```

### **2. API Authentication**
```python
# Require API keys
@router.post("/api/ingest/alert")
async def ingest_alert(
    alert: dict,
    api_key: str = Header(None, alias="X-API-Key")
):
    if not validate_api_key(api_key):
        raise HTTPException(401, "Invalid API key")

    # Process alert
```

### **3. Encryption**
- TLS 1.3 for all communications
- Encrypt database at rest
- Encrypt Kafka messages
- Secure credential storage (HashiCorp Vault)

---

## **Cost Estimates**

### **Small Office (Cloud VM)**
- AWS t3.large: $60/month
- Database (RDS): $40/month
- **Total: ~$100/month**

### **Enterprise (On-Premise)**
- 3x Dell servers: $15,000 (one-time)
- Maintenance: $200/month
- **Total: $15,000 + $200/month**

### **Enterprise (Cloud)**
- EKS Cluster: $300/month
- Database: $150/month
- Load Balancer: $50/month
- **Total: ~$500/month**

---

## **Next Steps for Your Office**

1. ‚úÖ Test current system with sample data
2. ‚úÖ Identify your log sources (firewalls, AD, EDR)
3. ‚úÖ Choose deployment scenario (1, 2, or 3)
4. ‚úÖ Set up log forwarding
5. ‚úÖ Deploy AI system
6. ‚úÖ Configure alerting (email/Slack)
7. ‚úÖ Train SOC team
8. ‚úÖ Monitor and tune

---

**Created by Kavi** üõ°Ô∏è

For questions or support, contact your IT security team.
